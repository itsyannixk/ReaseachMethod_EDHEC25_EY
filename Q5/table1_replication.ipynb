{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table 1 Replication - Leverage Analysis (1965-2003)\n",
        "\n",
        "This notebook replicates **Table 1** from the paper, showing descriptive statistics for **All Firms** and **Survivors** using Compustat data.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# STEP 0 — Environment Setup\n",
        "# Install required packages: wrds, pandas, numpy, scipy, jupyter, ipykernel\n",
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import wrds\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import os\n",
        "\n",
        "# Display settings for better output\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 3)\n",
        "pd.set_option('display.width', 120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1 — Connect to WRDS\n",
        "\n",
        "Establish connection to WRDS database. You'll be prompted for username and password.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to WRDS (enter credentials when prompted)\n",
        "db = wrds.Connection()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sanity check: verify 'comp' library is available\n",
        "libraries = db.list_libraries()\n",
        "print(f\"Available libraries: {len(libraries)}\")\n",
        "print(f\"'comp' available: {'comp' in libraries}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2 — Pull Compustat Data\n",
        "\n",
        "Pull annual Compustat data matching Section I of the paper:\n",
        "- **Nonfinancial firms** (SIC codes outside 6000-6999)\n",
        "- 1965–2003\n",
        "- Consolidated, domestic, INDL format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 2 — Pull Compustat Data\n",
        "# Pull annual Compustat data matching Section I: nonfinancial firms, 1965-2003, consolidated domestic\n",
        "# Note: Excludes financial firms (SIC 6000-6999) directly in SQL\n",
        "\n",
        "# Option: Load existing data to skip download\n",
        "if os.path.exists('data/01_raw_data.csv'):\n",
        "    print(\"Loading existing raw data from data/01_raw_data.csv\")\n",
        "    df = pd.read_csv('data/01_raw_data.csv', parse_dates=['datadate'])\n",
        "    print(f\"Loaded {len(df):,} observations\")\n",
        "else:\n",
        "    print(\"No existing data found, downloading from WRDS...\")\n",
        "\n",
        "    sql = \"\"\"\n",
        "    SELECT\n",
        "        gvkey,\n",
        "        datadate,\n",
        "        fyear,\n",
        "        sich,      -- SIC code (historical SIC code column in funda table)\n",
        "        at,        -- total assets\n",
        "        dlc,       -- short-term debt\n",
        "        dltt,      -- long-term debt\n",
        "        sale,\n",
        "        oibdp,\n",
        "        ppent,\n",
        "        prcc_f,\n",
        "        csho,\n",
        "        pstkl,\n",
        "        txditc,\n",
        "        intan,\n",
        "        dvc\n",
        "    FROM comp.funda\n",
        "    WHERE indfmt = 'INDL'\n",
        "      AND datafmt = 'STD'\n",
        "      AND popsrc = 'D'\n",
        "      AND consol = 'C'\n",
        "      AND fyear BETWEEN 1965 AND 2003\n",
        "      AND sich IS NOT NULL\n",
        "      AND (sich < 6000 OR sich > 6999)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Downloading data from WRDS... (this may take a few minutes)\")\n",
        "    df = db.raw_sql(sql, date_cols=['datadate'])\n",
        "    print(f\"Downloaded {len(df):,} observations\")\n",
        "    print(f\"Unique firms (gvkeys): {df['gvkey'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional Python-side filter for financials (ensures exclusion even when loading from CSV)\n",
        "df = df[df['sich'].notna()]\n",
        "df = df[(df['sich'] < 6000) | (df['sich'] > 6999)]\n",
        "print(f\"After excluding financials (SIC 6000-6999): {len(df):,} observations\")\n",
        "print(f\"Unique firms: {df['gvkey'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save raw downloaded data\n",
        "os.makedirs('data', exist_ok=True)\n",
        "df.to_csv('data/01_raw_data.csv', index=False)\n",
        "print(f\"✓ Saved raw data: {len(df):,} observations to data/01_raw_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the raw data\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3 — Basic Cleaning\n",
        "\n",
        "Apply paper's cleaning rules:\n",
        "1. Require non-missing, positive assets\n",
        "2. Fill missing debt components with zero\n",
        "3. Calculate total debt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Before cleaning: {len(df):,} observations\")\n",
        "\n",
        "# Require non-missing, positive assets\n",
        "df = df[df['at'].notna() & (df['at'] > 0)]\n",
        "print(f\"After asset filter: {len(df):,} observations\")\n",
        "\n",
        "# Replace missing debt components with 0\n",
        "df['dlc'] = df['dlc'].fillna(0)\n",
        "df['dltt'] = df['dltt'].fillna(0)\n",
        "\n",
        "# Total debt\n",
        "df['debt'] = df['dlc'] + df['dltt']\n",
        "\n",
        "print(f\"Debt computed for all {len(df):,} observations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned data\n",
        "df.to_csv('data/02_cleaned_data.csv', index=False)\n",
        "print(f\"✓ Saved cleaned data: {len(df):,} observations to data/02_cleaned_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4 — Construct Leverage Measures\n",
        "\n",
        "Calculate book and market leverage as defined in the Appendix:\n",
        "- **Book leverage** = Total Debt / Total Assets\n",
        "- **Market leverage** = Total Debt / (Total Debt + Market Equity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Book leverage\n",
        "df['book_lev'] = df['debt'] / df['at']\n",
        "\n",
        "# Market equity\n",
        "df['me'] = df['prcc_f'] * df['csho']\n",
        "\n",
        "# Market leverage\n",
        "df['market_lev'] = df['debt'] / (df['debt'] + df['me'])\n",
        "\n",
        "print(f\"Before leverage filter: {len(df):,} observations\")\n",
        "\n",
        "# Keep leverage in [0,1]\n",
        "df = df[\n",
        "    (df['book_lev'].between(0, 1)) &\n",
        "    (df['market_lev'].between(0, 1))\n",
        "]\n",
        "\n",
        "print(f\"After leverage filter [0,1]: {len(df):,} observations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save leverage data\n",
        "df.to_csv('data/03_leverage_data.csv', index=False)\n",
        "print(f\"✓ Saved leverage data: {len(df):,} observations to data/03_leverage_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 5 — Construct Table 1 Variables\n",
        "\n",
        "Create all variables that appear in Table 1:\n",
        "- Log sales (firm size) — set to NaN for non-positive sales\n",
        "- Market-to-book ratio\n",
        "- Profitability\n",
        "- Tangibility\n",
        "- Intangibles\n",
        "- Dividend payer dummy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log sales (proxy for firm size)\n",
        "# Set to NaN for non-positive sales (don't clip to 1, that creates fake values)\n",
        "df['log_sales'] = np.where(df['sale'] > 0, np.log(df['sale']), np.nan)\n",
        "\n",
        "# Market-to-book ratio\n",
        "df['mtb'] = (\n",
        "    df['me']\n",
        "    + df['debt']\n",
        "    + df['pstkl'].fillna(0)\n",
        "    - df['txditc'].fillna(0)\n",
        ") / df['at']\n",
        "\n",
        "# Profitability (EBITDA / Assets)\n",
        "df['profitability'] = df['oibdp'] / df['at']\n",
        "\n",
        "# Tangibility (PPE / Assets)\n",
        "df['tangibility'] = df['ppent'] / df['at']\n",
        "\n",
        "# Intangibles (Intangible Assets / Assets)\n",
        "df['intangibles'] = df['intan'] / df['at']\n",
        "\n",
        "# Dividend payer (binary: 1 if pays dividend, 0 otherwise)\n",
        "df['div_payer'] = (df['dvc'].fillna(0) > 0).astype(int)\n",
        "\n",
        "print(\"All Table 1 variables constructed\")\n",
        "print(f\"log_sales non-missing: {df['log_sales'].notna().sum():,} ({100*df['log_sales'].notna().mean():.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save variables data\n",
        "df.to_csv('data/04_variables_data.csv', index=False)\n",
        "print(f\"✓ Saved variables data: {len(df):,} observations to data/04_variables_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 6 — Cash-Flow Volatility\n",
        "\n",
        "Calculate rolling 3-year standard deviation of **scaled** operating income (oibdp/at).\n",
        "\n",
        "Using raw oibdp leads to scale issues (large firms have large volatility in dollar terms). Scaling by assets makes this a profitability-type measure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort by firm and year\n",
        "df = df.sort_values(['gvkey', 'fyear'])\n",
        "\n",
        "# Scaled cash flow (oibdp / assets)\n",
        "df['cf_base'] = df['oibdp'] / df['at']\n",
        "\n",
        "# Rolling 3-year standard deviation of SCALED operating income\n",
        "df['cf_vol'] = (\n",
        "    df.groupby('gvkey')['cf_base']\n",
        "      .rolling(window=3, min_periods=3)\n",
        "      .std()\n",
        "      .reset_index(level=0, drop=True)\n",
        ")\n",
        "\n",
        "print(f\"Cash-flow volatility computed for {df['cf_vol'].notna().sum():,} observations\")\n",
        "print(f\"cf_vol summary: mean={df['cf_vol'].mean():.4f}, median={df['cf_vol'].median():.4f}, std={df['cf_vol'].std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 7 — Industry Median Book Leverage\n",
        "\n",
        "Calculate industry median leverage using 2-digit SIC codes.\n",
        "\n",
        "**Note:** This uses SIC-2 classification. The paper may use Fama-French 38 industry classification, which could lead to minor differences in `ind_med_lev`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create 2-digit SIC code (using sich column from Compustat)\n",
        "df['sic2'] = df['sich'] // 100\n",
        "\n",
        "# Industry median leverage (by SIC-2 and year)\n",
        "df['ind_med_lev'] = (\n",
        "    df.groupby(['sic2', 'fyear'])['book_lev']\n",
        "      .transform('median')\n",
        ")\n",
        "\n",
        "print(f\"Industry median leverage computed\")\n",
        "print(f\"Number of unique industries (SIC-2): {df['sic2'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final data (no trimming applied to rows - trimming done in summary stats)\n",
        "df.to_csv('data/05_final_data.csv', index=False)\n",
        "print(f\"✓ Saved final data: {len(df):,} observations to data/05_final_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 8 — Define Survivors\n",
        "\n",
        "**Survivors** are defined as firms with ≥20 years of book leverage data in the sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count years of book leverage data per firm\n",
        "lev_counts = df.groupby('gvkey')['book_lev'].count()\n",
        "\n",
        "# Survivors: firms with ≥20 years\n",
        "survivors = lev_counts[lev_counts >= 20].index\n",
        "\n",
        "total_firms = df['gvkey'].nunique()\n",
        "print(f\"Total unique firms: {total_firms:,}\")\n",
        "print(f\"Survivors (≥20 years): {len(survivors):,}\")\n",
        "if total_firms > 0:\n",
        "    print(f\"Survivor rate: {100 * len(survivors) / total_firms:.1f}%\")\n",
        "else:\n",
        "    print(\"Survivor rate: N/A (no firms in dataset)\")\n",
        "\n",
        "# Create two datasets\n",
        "df_all = df.copy()\n",
        "df_surv = df[df['gvkey'].isin(survivors)].copy()\n",
        "\n",
        "print(f\"\\nAll Firms dataset: {len(df_all):,} observations\")\n",
        "print(f\"Survivors dataset: {len(df_surv):,} observations\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 9 — Replicate Table 1 Statistics\n",
        "\n",
        "Generate descriptive statistics (Mean, Median, SD) for both **All Firms** and **Survivors**.\n",
        "\n",
        "**Trimming approach:** Rather than dropping rows from the dataset, we compute trimmed statistics (1st/99th percentile) per variable within the summary function. This preserves sample size while handling outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variables to include in Table 1\n",
        "vars_table1 = [\n",
        "    'book_lev', 'market_lev', 'log_sales', 'mtb', 'profitability',\n",
        "    'tangibility', 'cf_vol', 'ind_med_lev', 'div_payer', 'intangibles'\n",
        "]\n",
        "\n",
        "def trimmed_stats(s, q=0.01):\n",
        "    \"\"\"\n",
        "    Compute mean, median, std after trimming tails at q and 1-q quantiles.\n",
        "    This is applied per variable, not across the entire dataset.\n",
        "    \"\"\"\n",
        "    s = s.dropna()\n",
        "    if s.empty or len(s) < 10:\n",
        "        return (np.nan, np.nan, np.nan)\n",
        "    lo, hi = s.quantile([q, 1-q])\n",
        "    s_trimmed = s[(s >= lo) & (s <= hi)]\n",
        "    return (s_trimmed.mean(), s_trimmed.median(), s_trimmed.std())\n",
        "\n",
        "def summary_table(data, q=0.01):\n",
        "    \"\"\"\n",
        "    Generate summary statistics table with trimmed moments.\n",
        "    Each variable is trimmed independently at 1%/99% by default.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for v in vars_table1:\n",
        "        mean, med, sd = trimmed_stats(data[v], q=q)\n",
        "        out[v] = {'Mean': mean, 'Median': med, 'SD': sd}\n",
        "    return pd.DataFrame(out).T\n",
        "\n",
        "# Generate tables\n",
        "table_all = summary_table(df_all)\n",
        "table_surv = summary_table(df_surv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TABLE 1 REPLICATION: ALL FIRMS\")\n",
        "print(\"=\"*80)\n",
        "print(table_all.round(3))\n",
        "print(f\"\\nNumber of observations: {len(df_all):,}\")\n",
        "print(f\"Number of unique firms: {df_all['gvkey'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"TABLE 1 REPLICATION: SURVIVORS\")\n",
        "print(\"=\"*80)\n",
        "print(table_surv.round(3))\n",
        "print(f\"\\nNumber of observations: {len(df_surv):,}\")\n",
        "print(f\"Number of unique firms: {df_surv['gvkey'].nunique():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 10 — Comparison: All Firms vs Survivors\n",
        "\n",
        "Compare the two groups side-by-side to highlight differences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create side-by-side comparison\n",
        "comparison = pd.DataFrame({\n",
        "    'All_Mean': table_all['Mean'],\n",
        "    'All_Median': table_all['Median'],\n",
        "    'Surv_Mean': table_surv['Mean'],\n",
        "    'Surv_Median': table_surv['Median'],\n",
        "    'Diff_Mean': table_surv['Mean'] - table_all['Mean']\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPARISON: ALL FIRMS vs SURVIVORS\")\n",
        "print(\"=\"*80)\n",
        "print(comparison.round(3))\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"- Survivors are LARGER (higher log_sales)\")\n",
        "print(\"- Survivors are MORE PROFITABLE (higher profitability)\")\n",
        "print(\"- Survivors are MORE TANGIBLE (higher tangibility)\")\n",
        "print(\"- Survivors have LOWER GROWTH (lower mtb)\")\n",
        "print(\"- Survivors are MORE LEVERED (higher book & market leverage)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results\n",
        "\n",
        "Save the results to CSV files for further analysis or reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export summary tables\n",
        "table_all.to_csv('table1_all_firms.csv')\n",
        "table_surv.to_csv('table1_survivors.csv')\n",
        "comparison.to_csv('table1_comparison.csv')\n",
        "\n",
        "print(\"Results exported to CSV files!\")\n",
        "print(\"  - table1_all_firms.csv\")\n",
        "print(\"  - table1_survivors.csv\")\n",
        "print(\"  - table1_comparison.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close the database connection\n",
        "db.close()\n",
        "print(\"WRDS connection closed.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
